{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a6a170",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "- Anomaly detection is a data analysis technique used to identify patterns, outliers, or unusual behaviors in a dataset. Its purpose is to distinguish rare or abnormal instances from the majority of normal data points, aiding in detecting potential fraud, errors, or unusual events that may require attention or investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e295a",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n",
    "- The key challenges in anomaly detection include dealing with imbalanced datasets where normal instances vastly outnumber anomalies, defining appropriate thresholds for anomaly detection, and adapting to dynamic and evolving patterns in data, which may require continuous model updates and retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af1129",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "- Unsupervised anomaly detection involves identifying anomalies in a dataset without using labeled examples of anomalies during training. It relies on identifying patterns that deviate significantly from the majority of data points. On the other hand, supervised anomaly detection requires labeled examples of both normal and anomalous instances during training, allowing the model to learn the distinction between the two classes and make predictions accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e504d0",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "The main categories of anomaly detection algorithms are:\n",
    "\n",
    "Statistical Methods: These algorithms use statistical techniques to model the normal behavior of the data and identify instances that deviate significantly from the expected patterns.\n",
    "\n",
    "Machine Learning-Based Methods: These algorithms use machine learning techniques, such as clustering, classification, or density estimation, to identify anomalies based on the learned representations of normal data.\n",
    "\n",
    "Deep Learning-Based Methods: These algorithms leverage deep neural networks to automatically learn complex representations of data and detect anomalies based on the deviations from the learned representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e816d6",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "- Distance-based anomaly detection methods assume that normal data points are tightly clustered together in the feature space, while anomalies are located far away from the normal cluster. These methods rely on the notion that the distance between data points can effectively capture the similarity or dissimilarity between instances, enabling the identification of outliers based on their distances from the majority of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab61b3d",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "- The LOF (Local Outlier Factor) algorithm computes anomaly scores by measuring the local density deviation of a data point compared to its neighboring data points. It calculates the ratio of the average local density of a data point's k-nearest neighbors to its own local density. Data points with significantly lower density compared to their neighbors are assigned higher LOF scores, indicating a higher likelihood of being anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29776af2",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "Number of Trees (n_estimators): This parameter determines the number of isolation trees to build, and increasing the number of trees can improve the accuracy of the anomaly detection.\n",
    "\n",
    "Subsample Size (max_samples): It controls the number of data points sampled to build each isolation tree, influencing the randomness and diversity of the trees.\n",
    "\n",
    "Contamination: This parameter sets the expected proportion of anomalies in the dataset, helping to adjust the decision boundary for classifying data points as anomalies or normal.\n",
    "\n",
    "Max Features: It determines the number of features to consider when splitting a node during tree construction, contributing to the randomness and diversity of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254dadf",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\n",
    "\n",
    "- In the KNN (K-Nearest Neighbors) algorithm, the anomaly score for a data point is calculated based on the distance to its k-nearest neighbors. If a data point has only 2 neighbors of the same class within a radius of 0.5, and K=10, then its anomaly score would be relatively high since it is significantly different from the majority of its neighbors. The fact that it has a low number of neighbors and that they all belong to the same class makes it stand out and raises suspicion of being an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dc371",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?\n",
    "\n",
    "- In the Isolation Forest algorithm, the anomaly score for a data point is calculated as the average path length it takes to isolate the data point across all the trees in the forest. A lower average path length indicates that the data point is easier to isolate and, therefore, more likely to be an anomaly. In this case, the data point with an average path length of 5.0 would have a relatively lower anomaly score compared to data points with longer average path lengths, suggesting it is less likely to be an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae864181",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf91982b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ddb0fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
