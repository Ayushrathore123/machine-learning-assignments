{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a65db2",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "\n",
    "- K-Nearest Neighbors (KNN) is a simple and intuitive machine learning algorithm used for classification and regression tasks. It works by finding the 'k' closest data points to a given query point and classifies or predicts the target value based on the majority class or average of the 'k' neighbors. KNN is a non-parametric algorithm, meaning it doesn't make any assumptions about the underlying data distribution, making it versatile and easy to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21e81e",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "- Choosing the value of 'K' in K-Nearest Neighbors (KNN) is crucial for the model's performance. A small 'K' value may lead to noisy predictions, while a large 'K' value may cause oversmoothing and loss of local patterns. Typically, the optimal 'K' is selected through hyperparameter tuning using techniques like cross-validation, where various 'K' values are tested, and the one that yields the best performance on validation data is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce09d4",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "The main difference between KNN classifier and KNN regressor lies in their application and output.\n",
    "\n",
    "KNN Classifier: It is used for classification tasks, where the algorithm predicts the class label of a new data point based on the majority class of its 'k' nearest neighbors. The output is a discrete class label.\n",
    "\n",
    "KNN Regressor: It is used for regression tasks, where the algorithm predicts the continuous value of a new data point by calculating the average (or weighted average) of the target values of its 'k' nearest neighbors. The output is a continuous numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff40afd",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "\n",
    "The performance of K-Nearest Neighbors (KNN) can be measured using various evaluation metrics depending on the task (classification or regression):\n",
    "\n",
    "Classification: Common performance metrics for KNN classification include accuracy, precision, recall, F1-score, and area under the Receiver Operating Characteristic curve (AUC-ROC). These metrics assess the model's ability to correctly classify instances into their respective classes.\n",
    "\n",
    "Regression: For KNN regression, metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared (coefficient of determination) are commonly used to evaluate the model's ability to predict continuous numerical values accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a57fc",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "- The curse of dimensionality in K-Nearest Neighbors (KNN) refers to the adverse effect of having a high number of features or dimensions in the dataset. As the number of dimensions increases, the data becomes sparse in the feature space, making it challenging for KNN to find meaningful nearest neighbors. Consequently, the algorithm's performance may degrade as the number of dimensions increases, and it may become computationally inefficient due to the increased search space and memory requirements. Feature selection or dimensionality reduction techniques are often employed to mitigate this issue and improve the efficiency and effectiveness of KNN in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607c20d",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Imputation: Before applying KNN, missing values can be imputed using techniques like mean, median, mode, or other statistical measures calculated from available data points. This ensures that the missing values are replaced with reasonable estimates and reduces their impact on the distance calculation during KNN.\n",
    "\n",
    "Feature Selection: If a significant number of instances have missing values for a particular feature, you may consider removing that feature from the dataset altogether. This can prevent the missing values from adversely affecting the distance computation during KNN.\n",
    "\n",
    "KNN Imputation: A specific variant of KNN called KNN imputation can be used to impute missing values. In this approach, missing values are filled by averaging the values of 'k' nearest neighbors with available data for each missing instance.\n",
    "\n",
    "Algorithm-Specific Methods: Some machine learning libraries may have built-in support for handling missing values in KNN. For example, in scikit-learn, you can use the 'KNNImputer' class, which performs KNN-based imputation for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4f3b2",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "The performance of the KNN classifier and regressor differs based on the type of problem they are applied to:\n",
    "\n",
    "KNN Classifier: The KNN classifier is suitable for classification tasks where the goal is to predict discrete class labels. It performs well when there is a clear separation between classes and when the decision boundaries are relatively simple.\n",
    "\n",
    "KNN Regressor: The KNN regressor is more appropriate for regression tasks where the objective is to predict continuous numerical values. It works well when there is a pattern of smoothness in the data, and the relationships between the features and target variable are continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc831dd0",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "- Simplicity and Intuition: KNN is easy to understand and implement, making it a straightforward choice for simple classification and regression tasks.\n",
    "- Non-parametric: KNN doesn't make assumptions about the underlying data distribution, allowing it to handle complex relationships and adapt to varying data patterns.\n",
    "- Few Hyperparameters: KNN mainly requires the 'K' value, making it less sensitive to tuning than some other algorithms.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "- Computational Complexity: As the dataset grows, the search for nearest neighbors becomes computationally expensive, leading to slow prediction times.\n",
    "- Sensitivity to Noise and Outliers: KNN can be sensitive to noisy data and outliers, impacting the quality of predictions.\n",
    "- Curse of Dimensionality: KNN's performance can degrade in high-dimensional spaces due to increased data sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c46d2",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "The main difference between Euclidean distance and Manhattan distance in K-Nearest Neighbors (KNN) lies in their calculation of distance between data points:\n",
    "\n",
    "Euclidean Distance: It measures the straight-line distance between two points in a multi-dimensional space. It calculates the square root of the sum of squared differences between corresponding features.\n",
    "\n",
    "Manhattan Distance: It calculates the distance by summing the absolute differences between corresponding features of two points. It represents the distance traveled along the axes of a grid-like path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21379ea3",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "The role of feature scaling in KNN is to ensure that all features contribute equally to the distance calculation. Feature scaling brings features to the same scale, preventing dominant features from overshadowing others during neighbor search and improving the performance of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e0b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5028b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d018d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf7cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f68cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6052aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c83908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae060d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c5792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aaf3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f5765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de5d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3fdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe49dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa1415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19a799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eda89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
